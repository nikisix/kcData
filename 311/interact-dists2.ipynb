{
 "metadata": {
  "name": "",
  "signature": "sha256:5f706af3bd727819448491e20c4b1007adcb02a958318316aab1394901a799ac"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Where will the next KC water pipe break?\n",
      "# \n",
      "# Bayesian Statistical Modeling using MCMC\n",
      "# by: Nick Tomasino\n",
      "\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.image as mpimg\n",
      "from matplotlib import animation\n",
      "from mpl_toolkits.basemap import Basemap\n",
      "\n",
      "import pymc as pm\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import pydot\n",
      "import scipy.misc\n",
      "\n",
      "# from pylab import *\n",
      "# JSAnimation import available at https://github.com/jakevdp/JSAnimation\n",
      "# from JSAnimation import IPython_display\n",
      "\n",
      "#d = pd.read_csv('data/311data') #full set\n",
      "d = pd.read_csv('data/311data', nrows=100) #limit to 1k rows for testing\n",
      "d = pd.DataFrame(d, columns=['creation_date', 'closed_date', 'latitude', 'longitude', 'request_type'])\n",
      "#d.sort(columns=['closed_date'], ascending=True, inplace=True) #do this after filtering to save on cycles\n",
      "\n",
      "#-1 is the value that find returns if it doesn't find the string\n",
      "#Filter the Dataset. Remove all rows without 'Water' in their request_type\n",
      "data = pd.DataFrame\n",
      "i = 0\n",
      "for index, row in d.iterrows():\n",
      "     if(\\\n",
      "       ( str.upper(row['request_type']).find('WATER') != -1 ) and\\\n",
      "       ( row['longitude']  !=0 ) and\\\n",
      "       ( row['latitude' ]  !=0 ) ):            \n",
      "            row_frame = pd.DataFrame([{'creation_date':row.creation_date, 'closed_date':row.closed_date, 'longitude':row.longitude, 'latitude':row.latitude, 'request_type':row.request_type }])\n",
      "            if i == 0: \n",
      "                data = row_frame\n",
      "                i+=1\n",
      "            else: \n",
      "                data = data.append(row_frame)\n",
      "\n",
      "del d #relax d's memory usage\n",
      "data.sort(columns=['closed_date'], ascending=True, inplace=True)\n",
      "dt_index = pd.DatetimeIndex(pd.to_datetime(data.closed_date))\n",
      "data = data.set_index(dt_index)\n",
      "\n",
      "\"\"\"\n",
      "LAT_BEGIN = 38.75\n",
      "LAT_END   = 39.5\n",
      "LON_BEGIN = -96\n",
      "LON_END   = -95\n",
      "\"\"\"\n",
      "LAT_BEGIN = 38.8\n",
      "LAT_END   = 39.35\n",
      "LON_BEGIN = -94.75\n",
      "LON_END   = -94.3\n",
      "N_STEPS   = 3\n",
      "lat_steps = np.linspace(LAT_BEGIN, LAT_END, N_STEPS)\n",
      "lon_steps = np.linspace(LON_BEGIN, LON_END, N_STEPS)\n",
      "\"\"\"\n",
      "the grid variable represents quadrants of latitude and longitude\n",
      "i.e. a 3x3 grid of squares at every point in time\n",
      "-95.0 38.75 - grid[0]\n",
      "-95.5 38.75 - grid[1]\n",
      "-96.0 38.75 - ...\n",
      "-95.0 39.125\n",
      "-95.5 39.125\n",
      "-96.0 39.125\n",
      "-95.0 39.5\n",
      "-95.5 39.5\n",
      "-96.0 39.5 - grid[8]\n",
      "\"\"\"\n",
      "#returns the grid indicies for a particular geo point\n",
      "#N_STEPS-1 => one to compensate the +1 lookahead\n",
      "def in_square(lat, lon, lat_steps, lon_steps):\n",
      "    if(lon <= lon_steps[0]):           lon_index = 0\n",
      "    elif(lon_steps[N_STEPS-1] <= lon): lon_index = N_STEPS-1\n",
      "    else:\n",
      "        for x in range(0, N_STEPS - 1):\n",
      "            if(lon_steps[x] <= lon <= lon_steps[x+1]):  lon_index = x\n",
      "   \n",
      "    if(lat <= lat_steps[0]):           lat_index = 0\n",
      "    elif(lat_steps[N_STEPS-1] <= lat): lat_index = N_STEPS-1            \n",
      "    else:\n",
      "        for y in range(0, N_STEPS - 1): \n",
      "            if(lat_steps[y] <= lat <= lat_steps[y+1]):  lat_index = y\n",
      "    return lat_index, lon_index\n",
      "\n",
      "\"\"\"grid_split [] [] / [] [] / [] []\n",
      "input:  lats - list of latitudes on day d\n",
      "        lons - list of longitudes on day d\n",
      "        \n",
      "output: count for each square\n",
      "[1] [3] / [2] [1] / [0] [1]\n",
      "\"\"\"\n",
      "def grid_split_2d(lat_steps, lon_steps, day_data):\n",
      "    grid = np.zeros((len(lat_steps), len(lon_steps)))\n",
      "    for i, row in day_data.iterrows():\n",
      "        lat_index, lon_index = in_square(row.latitude, row.longitude, lat_steps, lon_steps)\n",
      "        grid[lat_index, lon_index] += 1\n",
      "    return grid\n",
      "\n",
      "\"\"\"returns a 1d grid in array form\"\"\"\n",
      "def grid_split_1d(lat_steps, lon_steps, day_data):\n",
      "    grid = np.zeros((len(lat_steps) * len(lon_steps))) #changed to ndarray\n",
      "    for i, row in day_data.iterrows():\n",
      "        lat_index, lon_index = in_square(row.latitude, row.longitude, lat_steps, lon_steps)\n",
      "        grid[(N_STEPS*lat_index) + lon_index] += 1 #could also be len(lon_steps)*lat_index\n",
      "    return grid\n",
      "\n",
      "#uses the two functions above to generate a grid dictionary from the data frame\n",
      "def grid2d_from_data(data, grid2d = dict()):\n",
      "    for date_index in data.index:    \n",
      "        if (pd.isnull(date_index)): continue #last few rows are always NaT (Not a Time) :(\n",
      "        day_data = data[str(date_index)]\n",
      "        grid2d[date_index] = grid_split_2d(lat_steps, lon_steps, day_data)\n",
      "    return grid2d\n",
      "\n",
      "def grid1d_from_data(data, grid1d = dict()):\n",
      "    for date_index in data.index:    \n",
      "        if (pd.isnull(date_index)): continue #last few rows are always NaT (Not a Time) :(\n",
      "        day_data = data[str(date_index)]\n",
      "        grid1d[date_index] = grid_split_1d(lat_steps, lon_steps, day_data)\n",
      "    return grid1d\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "grid1d = dict()\n",
      "grid1d = grid1d_from_data(data, grid1d = grid1d)\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "grid2d = dict()\n",
      "grid2d = grid2d_from_data(data, grid2d=grid2d)\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "#uniform prior (all ones), either this or discrete_uniform\n",
      "#unif = pm.DiscreteUniform('unif', 0, 1, size=[1,N_STEPS**2] )\n",
      "#alpha = unif.random()\n",
      "alpha = np.ones(N_STEPS**2)\n",
      "dirich = pm.Dirichlet('dirich', theta = alpha)\n",
      "\n",
      "\n",
      "#initialize with n equal to the average of the daily count data\n",
      "total = sum(data.groupby('closed_date')['request_type'].count())\n",
      "num_elements = len(data.groupby('closed_date'))\n",
      "avg_calls_per_day = total / num_elements\n",
      "\n",
      "expon = pm.Exponential('expon', beta = avg_calls_per_day)\n",
      "\n",
      "#set n = to poison centered on n\n",
      "#poisson = pm.Poisson('poisson', mu = avg_calls_per_day)\n",
      "#poisson = pm.Poisson('poisson', mu = avg_calls_per_day, observed=True,\n",
      "#                      value = [ np.sum(grid1d.values()[i]) for i in range( 0, len(grid1d.values()) ) ])\n",
      "\n",
      "#\"value =\" is where the data comes in. adding up all values in the grid to get the poisson count for that day\n",
      "poisson = pm.Poisson('poisson', mu = expon, observed=True,\n",
      "                      value = [ np.sum(grid1d.values()[i]) for i in range( 0, len(grid1d.values()) ) ])\n",
      "\n",
      "\n",
      "#multi = pm.Multinomial('multi', n=poisson, p=dirich, value=grid1d.values(), observed=True) #want to do this\n",
      "#multi = pm.Multinomial('multi', n=poisson, p=dirich) #works\n",
      "#multi = pm.Multinomial('multi', n=poisson, p=dirich, value=[0,0,0,0,0,0,1,1,1], observed=True) #works when n == sum(value)\n",
      "\n",
      "\"\"\"\n",
      "multi = pm.Multinomial('multi', p=dirich, observed=True, \n",
      "                        n = [ np.sum(grid1d.values()[i]) for i in range( 0, len(grid1d.values()) ) ] , \n",
      "                        value = [ grid1d.values()[i] for i in range( 0, len(grid1d.values()) ) ] )\n",
      "model = pm.Model([multi, dirich], name = 'model')\n",
      "\"\"\"\n",
      "\n",
      "#similar to the poisson's 'value =' except each grid get's it's own daily count, \n",
      "#instead of adding all of the grid-squares together\n",
      "multi = pm.Multinomial('multi', p=dirich, observed=True, \n",
      "                        n =  poisson, \n",
      "                        value = [ grid1d.values()[i] for i in range( 0, len(grid1d.values()) ) ] )\n",
      "model = pm.Model([multi, dirich, poisson, expon], name = 'model')\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "mcmc = pm.MCMC(model)\n",
      "mcmc.sample(200, 100, 1)\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "dirich_samples = mcmc.trace('dirich')[:]\n",
      "expon_sapmples = mcmc.trace('expon')[:]\n",
      "#no samples from these last two b/c they're observed\n",
      "#poisson_samples = mcmc.trace('poisson1')[:]\n",
      "#multi_samples = mcmc.trace('multi')[:]\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "#create new model of with unobserved data using our _posteriors_ from MCMC above\n",
      "#sampling from this model is like sampling an artificial dataset tuned to our data\n",
      "poisson1 = pm.Poisson('poisson1', mu = expon)\n",
      "\n",
      "multi1 = pm.Multinomial('multi1', n=poisson1, p=dirich) #works\n",
      "\n",
      "model1 = pm.Model([multi1, dirich, poisson1, expon], name = 'model1')\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "mcmc1 = pm.MCMC(model1)\n",
      "mcmc1.sample(200, 100, 1)\n",
      "#dirich_samples = mcmc.trace('dirich')[:]\n",
      "#expon_sapmples = mcmc.trace('expon')[:]\n",
      "\n",
      "#CAN sample from these now b/c they're observed\n",
      "poisson1_samples = mcmc1.trace('poisson1')[:]\n",
      "multi1_samples = mcmc1.trace('multi1')[:]\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "def add_noise(multi_sample):\n",
      "    normal = pm.Normal(name='normal', mu=0, tau=1, size=len(multi_sample))\n",
      "    sample = normal.random() + multi_sample\n",
      "    return [ \n",
      "       round(sample[i]) \n",
      "       if round(sample[i]) > 0\n",
      "       else 0.0\n",
      "       for i in range(len(sample))\n",
      "    ]\n",
      "\n",
      "def condition_dict_grid(grid, sector, num_calls):\n",
      "    ret = list()\n",
      "    for sectors in grid.itervalues():\n",
      "        #sector we got a call in matches the number of calls we're looking for\n",
      "        if sectors[sector] == num_calls: \n",
      "            ret.append(sectors)\n",
      "    return ret\n",
      "\n",
      "#TODO change grid1d from a dict to a list and combine these functions\n",
      "#returns a subset of grid with rows whose sector==num_calls \n",
      "def condition_grid(grid, sector, num_calls):\n",
      "    ret = list()\n",
      "    for sectors in grid:\n",
      "        #sector we got a call in matches the number of calls we're looking for\n",
      "        if sectors[sector] == num_calls: \n",
      "            ret.append(sectors)\n",
      "    return ret\n",
      "\n",
      "\"\"\"generates an artificial datapoint \n",
      "(by using the posteriors from mcmc for dirichlet and exponential)\n",
      "multi1.random()\n",
      "\n",
      "min support - minimum number of points needed for a reasonable prediction to be made\n",
      "if there aren't enough points in the real dataset, then we sample from\n",
      "the unobserved_multinomial to augment our artificial dataset\"\"\"\n",
      "\n",
      "def augment_grid(sector, num_calls, samples_per_iter = 100, min_support = 20, grid_real = list(), MAX_SAMPLES=20000):\n",
      "    grid1d_artificial = []\n",
      "    conditioned_grid_real = condition_grid( grid_real, sector, num_calls)\n",
      "    if len(conditioned_grid_real) >= min_support:\n",
      "        return conditioned_grid_real\n",
      "    else:\n",
      "        iters = 0\n",
      "        while ( len(grid1d_artificial) < (min_support - len(conditioned_grid_real)) ) and (iters*samples_per_iter < MAX_SAMPLES):\n",
      "            artificial_samples = [add_noise(multi1.random()) for i in range(samples_per_iter)]\n",
      "            iters+=1\n",
      "            if np.mod(iters, 100) == 0: print 'sample iter 100 x ', iters\n",
      "            grid1d_artificial.extend(  condition_grid( artificial_samples, sector, num_calls )  )\n",
      "    return grid1d_artificial\n",
      "\n",
      "\"\"\"\n",
      "GRIDSQUARES -> LATS, LONS\n",
      ".--> (x) == lon_steps\n",
      "| \n",
      "v (y) == lat_steps\n",
      "\n",
      "lats and lons were hashed into grid squares via the gridsplit1d and in_square functions\n",
      "grid[(N_STEPS*lat_index) + lon_index] += 1\n",
      "\"\"\"\n",
      "\n",
      "lat_stepsize = np.abs(lat_steps[0] - lat_steps[1])\n",
      "lon_stepsize = np.abs(lon_steps[0] - lon_steps[1])\n",
      "\n",
      "lat_sd = lat_stepsize**2 if lat_stepsize < 1 else np.sqrt(lat_stepsize)\n",
      "lon_sd = lon_stepsize**2 if lon_stepsize < 1 else np.sqrt(lon_stepsize)\n",
      "\n",
      "def grid1dToLatsLons_plot(grid):\n",
      "    lats = []; lons = [];\n",
      "    for i in range( len( grid) ):\n",
      "        for j in range( len( grid[i] ) ):\n",
      "            num_calls = grid[i][j]\n",
      "            if num_calls == 0: continue\n",
      "            lat_step = np.mod(j, N_STEPS)\n",
      "            lon_step = j / N_STEPS\n",
      "            \n",
      "            lat_mean = lat_steps[lat_step] \n",
      "            lon_mean = lon_steps[lon_step]               \n",
      "            #lats.append(lat_mean) #works\n",
      "            #lons.append(lon_mean)\n",
      "            \n",
      "            lats.extend(np.random.normal(loc=lat_mean, scale=lat_sd, size = num_calls )) \n",
      "            lons.extend(np.random.normal(loc=lon_mean, scale=lat_sd, size = num_calls )) \n",
      "    return lats, lons\n",
      "\n",
      "#for use with plt.scatter(). \n",
      "def grid1dToLatsLonsSize_scatter(grid):\n",
      "    lats = []; lons = []; sizes=[]\n",
      "    for i in range( len( grid) ):\n",
      "        for j in range( len( grid[i] ) ):\n",
      "            num_calls = grid[i][j]\n",
      "            if num_calls == 0: continue\n",
      "            lat_step = np.mod(j, N_STEPS)\n",
      "            lon_step = j / N_STEPS\n",
      "            \n",
      "            #if (lat_step == 0 and lon_step==0):\n",
      "            lat_mean = lat_steps[lat_step]\n",
      "            lon_mean = lon_steps[lon_step]\n",
      "            \n",
      "            lats.append(np.random.normal(loc=lat_mean, scale=lat_sd ))\n",
      "            lons.append(np.random.normal(loc=lon_mean, scale=lat_sd ))\n",
      "            sizes.append(num_calls)\n",
      "    return lats, lons, sizes\n",
      "\n",
      "geo_map = Basemap(projection='merc', lat_0=39, lon_0=-94.5,\n",
      "    resolution = 'l', area_thresh = 3000.0,\n",
      "    llcrnrlon=-95, llcrnrlat=38.7,\n",
      "    urcrnrlon=-94, urcrnrlat=39.6)\n",
      "\n",
      "def sample_points(sector = 1, num_calls = 2, grid = grid1d):\n",
      "    conditioned_grid = condition_dict_grid(grid1d, sector=sector, num_calls = num_calls)\n",
      "    cgrid_upsampled = augment_grid( sector, num_calls, grid_real= conditioned_grid)\n",
      "\n",
      "    lats, lons, sizes = grid1dToLatsLonsSize_scatter(conditioned_grid)\n",
      "    xpts, ypts = geo_map(lons, lats)\n",
      "\n",
      "    lats, lons, sizes = grid1dToLatsLonsSize_scatter(cgrid_upsampled)\n",
      "    xpts_sampled, ypts_sampled = geo_map(lons, lats)\n",
      "    return xpts, ypts, xpts_sampled, ypts_sampled\n",
      "\n",
      "def inc_dict(dictionary, key):\n",
      "    if dictionary.__contains__(key):     dictionary[key] += 1\n",
      "    else: dictionary[key] = 1\n",
      "    \n",
      "call_dict={}\n",
      "call_dict[1]=2 #keeps track of sector and num_calls on the grid\n",
      "\n",
      "xpts, ypts, xpts_sampled, ypts_sampled = sample_points(sector=1, num_calls=2, grid=grid1d)\n",
      "\n",
      "\"\"\"\n",
      "Graph a heatmap of the data by sector\n",
      "May be conditioned on getting an exact number of calls in a sector\n",
      "\"\"\"\n",
      "\n",
      "from matplotlib.widgets import Button\n",
      "\n",
      "plt.figure(figsize=(24,12))\n",
      "plt.axes().set_axis_bgcolor('grey')\n",
      "img=mpimg.imread('kcmo1.png')\n",
      "left = np.min(xpts_sampled); right = np.max(xpts_sampled); bottom = np.min(ypts_sampled); top = np.max(ypts_sampled)\n",
      "imgplot = plt.imshow(img, extent=[left, right, bottom, top])\n",
      "size = ( np.abs(right-left) + np.abs(top-bottom) ) / (2*20)\n",
      "real_points    = plt.axes().scatter(x=xpts, y=ypts, alpha=.4, s=size, c='cyan', label='real data')\n",
      "sampled_points = plt.axes().scatter(x=xpts_sampled, y=ypts_sampled, alpha=.1, s=size, c = 'violet', label='sampled data')\n",
      "plt.axes().legend(('real data', 'sampled data'), loc='lower right', markerscale=.2, framealpha=.7 )\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print sampled_points.get_array()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "[]"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Reset Button\n",
      "#*rect* = [left, bottom, width, height] \n",
      "reset_axis = plt.axes([0.4, 0.15, 0.1, 0.04], alpha = .2) #TODO alpha here not working \n",
      "reset_button = Button(ax=reset_axis, label='Reset', color='lightblue' , hovercolor='0.975') \n",
      "\n",
      "def clear_points():\n",
      "    if real_points: real_points.remove()\n",
      "    if sampled_points: sampled_points.remove()\n",
      "    \n",
      "def reset(event):                                                               \n",
      "    clear_points()\n",
      "    call_dict.clear()\n",
      "    plt.show()\n",
      "    #plt.clf() #clears whole figure\n",
      "\n",
      "reset_button.on_clicked(reset)\n",
      "\n",
      "#Button: add to square 1\n",
      "axis1 = plt.axes([0.2, 0.35, 0.1, 0.04], alpha = .2) \n",
      "button1 = Button(ax=axis1, label='Inc1', color='lightblue' , hovercolor='0.975') \n",
      "\n",
      "def add1(event):                                                               \n",
      "    clear_points()\n",
      "    inc_dict(call_dict, key=1)\n",
      "    xpts, ypts, xpts_sampled, ypts_sampled = sample_points(sector=1, num_calls=call_dict[1], grid=grid1d)\n",
      "    real_points    = plt.axes().scatter(x=xpts, y=ypts, alpha=.4, s=size, c='cyan', label='real data')\n",
      "    sampled_points = plt.axes().scatter(x=xpts_sampled, y=ypts_sampled, alpha=.1, s=size, c = 'violet', label='sampled data')\n",
      "    plt.show()\n",
      "\n",
      "button1.on_clicked(add1)\n",
      "\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}